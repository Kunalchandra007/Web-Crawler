# ğŸ•·ï¸ Crawlerbykc - Open Source Web Crawler for Java
**Crawlerbykc** is a powerful, multi-threaded web crawler for Java applications. Originally based on crawler4j, it has been completely refactored and enhanced to provide a robust, scalable, and easy-to-use web crawling solution.

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [How It Works](#-how-it-works)
- [Usage Examples](#-usage-examples)
- [Configuration](#-configuration)
- [Project Structure](#-project-structure)
- [API Reference](#-api-reference)
- [Testing](#-testing)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

## âœ¨ Features

### ğŸš€ Core Capabilities
- **Multi-threaded Crawling**: Efficient concurrent web crawling
- **Robots.txt Compliance**: Respects website crawling rules
- **Politeness Delays**: Configurable delays to prevent server overload
- **Resumable Crawling**: Resume interrupted crawls
- **Binary Content Support**: Handles images, PDFs, and other content types
- **Authentication**: Supports Basic, Form, and NTLM authentication
- **Proxy Support**: Crawl behind proxies
- **SSL/HTTPS Support**: Secure crawling capabilities

### ğŸ›¡ï¸ Built-in Safety Features
- **Rate Limiting**: Prevents overwhelming target servers
- **Error Handling**: Robust error recovery and logging
- **Memory Management**: Efficient memory usage for large crawls
- **Timeout Management**: Configurable timeouts for all operations
- **Duplicate Detection**: Avoids crawling the same URL multiple times

### ğŸ”§ Developer-Friendly
- **Simple API**: Easy-to-use interface for custom crawlers
- **Extensible**: Create custom crawler implementations
- **Well-Documented**: Comprehensive documentation and examples
- **Maven/Gradle Support**: Easy integration into existing projects

## ğŸš€ Quick Start

### Prerequisites
- **Java 8 or higher** (tested with Java 8, 11, 17, 23)
- **Maven or Gradle** (for building)

### 1. Build the Project
```bash
# Clone the repository
git clone https://github.com/your-repo/crawlerbykc.git
cd crawlerbykc

# Build using Gradle
./gradlew build

# Or build using Maven
mvn clean install
```

### 2. Run Tests
```bash
# Run all tests
./gradlew test

# Run specific module tests
./gradlew :crawlerbykc:test
```

### 3. Run Demo
```bash
# Run the built-in demo
./gradlew :crawlerbykc:runDemo
```

## ğŸ“¦ Installation

### Maven
```xml
    <dependency>
        <groupId>edu.uci.ics</groupId>
    <artifactId>crawlerbykc</artifactId>
    <version>1.0.0-SNAPSHOT</version>
    </dependency>
```

### Gradle
```gradle
implementation 'edu.uci.ics:crawlerbykc:1.0.0-SNAPSHOT'
```

## ğŸ•·ï¸ How It Works

### Crawling Process Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Initializationâ”‚ -> â”‚   Seed URLs     â”‚ -> â”‚  Crawling Loop  â”‚
â”‚   & Config      â”‚    â”‚   Setup         â”‚    â”‚  (Fetch/Parse)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         v                       v                       v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HTTP Client    â”‚    â”‚   URL Queue     â”‚    â”‚  Content Store  â”‚
â”‚  Setup          â”‚    â”‚  (Frontier)     â”‚    â”‚  & Processing   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Process

1. **Initialization Phase**
   - Sets up crawling configuration (delays, max pages, depth, etc.)
   - Creates storage folder for crawl data
   - Initializes HTTP client and robots.txt checker

2. **Seed URL Processing**
   - Starts with one or more seed URLs (starting points)
   - Adds them to the crawl queue (Frontier)
   - Each URL gets a unique ID and priority

3. **Main Crawling Loop**
   - **Fetch**: Downloads web pages using HTTP client
   - **Parse**: Extracts HTML content, links, and metadata
   - **Filter**: Checks robots.txt compliance and URL filters
   - **Store**: Saves page data and discovered URLs
   - **Queue**: Adds new URLs to crawl queue

4. **Content Processing**
   - **HTML Parsing**: Extracts text, links, images, metadata
   - **Link Discovery**: Finds all outgoing links on the page
   - **Content Filtering**: Skips binary files (images, PDFs, etc.)
   - **Duplicate Detection**: Avoids crawling same URL twice

5. **Politeness & Rate Limiting**
   - Respects robots.txt rules
   - Implements delays between requests
   - Limits concurrent connections per domain

### What It Searches/Extracts
- **Text Content**: All readable text from web pages
- **Links**: All hyperlinks (internal and external)
- **Metadata**: Page titles, descriptions, keywords
- **Images**: Image URLs and alt text
- **Forms**: Form elements and input fields
- **Structured Data**: JSON-LD, microdata, etc.

## ğŸ’» Usage Examples

### Basic Crawler

```java
import edu.uci.ics.crawlerbykc.crawler.*;
import edu.uci.ics.crawlerbykc.fetcher.PageFetcher;
import edu.uci.ics.crawlerbykc.parser.HtmlParseData;
import edu.uci.ics.crawlerbykc.robotstxt.*;
import edu.uci.ics.crawlerbykc.url.WebURL;

import java.util.Set;
import java.util.regex.Pattern;

public class MyCrawler extends WebCrawler {

    private final static Pattern FILTERS = Pattern.compile(
        ".*(\\.(css|js|gif|jpg|png|mp3|mp4|zip|gz))$");

     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
         String href = url.getURL().toLowerCase();
        return !FILTERS.matcher(href).matches() && 
               href.startsWith("https://example.com/");
     }

     @Override
     public void visit(Page page) {
         String url = page.getWebURL().getURL();
        System.out.println("Crawled: " + url);

         if (page.getParseData() instanceof HtmlParseData) {
             HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
             String text = htmlParseData.getText();
             String html = htmlParseData.getHtml();
             Set<WebURL> links = htmlParseData.getOutgoingUrls();

            // Process the page content
            processPage(url, text, html, links);
        }
    }

    private void processPage(String url, String text, String html, Set<WebURL> links) {
        // Your custom processing logic here
        System.out.println("Text length: " + text.length());
        System.out.println("Links found: " + links.size());
    }

    public static void main(String[] args) throws Exception {
        String crawlStorageFolder = "/tmp/crawler/";
        int numberOfCrawlers = 7;

        CrawlConfig config = new CrawlConfig();
        config.setCrawlStorageFolder(crawlStorageFolder);
        config.setPolitenessDelay(1000);
        config.setMaxDepthOfCrawling(2);
        config.setMaxPagesToFetch(1000);

        PageFetcher pageFetcher = new PageFetcher(config);
        RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
        RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
        CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);

        controller.addSeed("https://example.com/");
        controller.start(MyCrawler::new, numberOfCrawlers);
    }
}
```

### Advanced Configuration

```java
CrawlConfig config = new CrawlConfig();

// Basic settings
config.setCrawlStorageFolder("/path/to/storage");
config.setPolitenessDelay(1000);        // 1 second delay
config.setMaxDepthOfCrawling(2);        // Max crawl depth
config.setMaxPagesToFetch(1000);        // Max pages to crawl
config.setUserAgentString("MyCrawler/1.0");

// Advanced settings
config.setResumableCrawling(true);      // Resume interrupted crawls
config.setMaxDownloadSize(1048576);     // Max page size (1MB)
config.setIncludeBinaryContentInCrawling(false); // Skip binary files
config.setMaxConnectionsPerHost(100);   // Max connections per host
config.setConnectionTimeout(20000);     // Connection timeout (20s)
config.setSocketTimeout(20000);         // Socket timeout (20s)

// Authentication
config.setAuthenticationInfo("username", "password");

// Proxy settings
config.setProxyHost("proxy.example.com");
config.setProxyPort(8080);
```

### Custom Data Storage

```java
public class DatabaseCrawler extends WebCrawler {
    
    private DatabaseService dbService;
    
    public DatabaseCrawler() {
        this.dbService = new DatabaseService();
    }
    
    @Override
    public void visit(Page page) {
        if (page.getParseData() instanceof HtmlParseData) {
            HtmlParseData htmlParseData = (HtmlParseData) page.getParseData();
            
            // Store in database
            dbService.savePage(
                page.getWebURL().getURL(),
                htmlParseData.getText(),
                htmlParseData.getHtml(),
                htmlParseData.getOutgoingUrls()
            );
        }
    }
}
```

## âš™ï¸ Configuration

### Basic Configuration Options

| Option | Description | Default |
|--------|-------------|---------|
| `crawlStorageFolder` | Directory to store crawl data | Required |
| `politenessDelay` | Delay between requests (ms) | 200 |
| `maxDepthOfCrawling` | Maximum crawl depth | -1 (unlimited) |
| `maxPagesToFetch` | Maximum pages to crawl | -1 (unlimited) |
| `userAgentString` | User agent string | "crawler4j" |
| `resumableCrawling` | Enable resumable crawling | false |

### Advanced Configuration Options

| Option | Description | Default |
|--------|-------------|---------|
| `maxDownloadSize` | Maximum page size (bytes) | 10485760 |
| `includeBinaryContentInCrawling` | Include binary files | false |
| `processBinaryContentInCrawling` | Process binary content | false |
| `maxConnectionsPerHost` | Max connections per host | 100 |
| `connectionTimeout` | Connection timeout (ms) | 20000 |
| `socketTimeout` | Socket timeout (ms) | 20000 |
| `proxyHost` | Proxy host | null |
| `proxyPort` | Proxy port | -1 |

### Environment Variables

```bash
# Java options for better performance
export JAVA_OPTS="-Xmx2g -XX:+UseG1GC"

# Gradle options
export GRADLE_OPTS="-Xmx1g"
```

## ğŸ“ Project Structure

```
crawlerbykc-parent/
â”œâ”€â”€ ğŸ“š docs/                          # Documentation
â”‚   â””â”€â”€ README.md                     # This file
â”œâ”€â”€ ğŸ³ docker/                        # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile                    # Docker image definition
â”‚   â”œâ”€â”€ docker-compose.yml            # Multi-container setup
â”‚   â””â”€â”€ docker-run.ps1                # Docker run script
â”œâ”€â”€ ğŸ“œ scripts/                       # Setup and utility scripts
â”‚   â”œâ”€â”€ setup-*.ps1                   # PowerShell setup scripts
â”‚   â””â”€â”€ setup-*.bat                   # Windows batch scripts
â”œâ”€â”€ ğŸ”¨ build-artifacts/               # Build outputs and configs
â”‚   â”œâ”€â”€ *.class                       # Compiled classes
â”‚   â””â”€â”€ local.properties              # Local Gradle properties
â”œâ”€â”€ ğŸ—‚ï¸ temp-files/                    # Temporary files
â”‚   â””â”€â”€ java8/                        # Java 8 installation
â”œâ”€â”€ ğŸ¢ crawlerbykc/                   # Main library module
â”‚   â”œâ”€â”€ src/main/java/edu/uci/ics/crawlerbykc/
â”‚   â”‚   â”œâ”€â”€ crawler/                  # Core crawling logic
â”‚   â”‚   â”œâ”€â”€ fetcher/                  # HTTP fetching
â”‚   â”‚   â”œâ”€â”€ frontier/                 # URL queue management
â”‚   â”‚   â”œâ”€â”€ parser/                   # Content parsing
â”‚   â”‚   â”œâ”€â”€ robotstxt/                # Robots.txt handling
â”‚   â”‚   â”œâ”€â”€ url/                      # URL processing
â”‚   â”‚   â””â”€â”€ util/                     # Utilities
â”‚   â””â”€â”€ build.gradle                  # Module build file
â”œâ”€â”€ ğŸ“‹ crawlerbykc-examples/          # Example applications
â”‚   â”œâ”€â”€ crawlerbykc-examples-base/    # Basic examples
â”‚   â””â”€â”€ crawlerbykc-examples-postgres/ # PostgreSQL examples
â”œâ”€â”€ âš™ï¸ config/                        # Configuration files
â”‚   â”œâ”€â”€ checkstyle.xml                # Code style rules
â”‚   â””â”€â”€ intellij/                     # IntelliJ settings
â”œâ”€â”€ ğŸ—ï¸ gradle/                        # Gradle wrapper
â”‚   â””â”€â”€ wrapper/
â”œâ”€â”€ ğŸ“„ build.gradle                   # Root build file
â”œâ”€â”€ ğŸ“„ settings.gradle                # Project settings
â”œâ”€â”€ ğŸ“„ gradle.properties              # Gradle properties
â”œâ”€â”€ ğŸ“„ gradlew                        # Gradle wrapper (Unix)
â””â”€â”€ ğŸ“„ gradlew.bat                    # Gradle wrapper (Windows)
```

## ğŸ“š API Reference

### Core Classes

#### CrawlController
Main orchestrator of the crawling process.

```java
CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);
controller.addSeed("https://example.com/");
controller.start(MyCrawler::new, numberOfCrawlers);
```

#### WebCrawler
Base class for custom crawler implementations.

```java
public class MyCrawler extends WebCrawler {
    @Override
    public boolean shouldVisit(Page referringPage, WebURL url) {
        // Return true if URL should be visited
    }
    
    @Override
    public void visit(Page page) {
        // Process the crawled page
    }
}
```

#### CrawlConfig
Configuration for crawling behavior.

```java
CrawlConfig config = new CrawlConfig();
config.setCrawlStorageFolder("/tmp/crawler");
config.setPolitenessDelay(1000);
config.setMaxPagesToFetch(1000);
```

### Data Classes

#### Page
Represents a fetched web page.

```java
String url = page.getWebURL().getURL();
ParseData parseData = page.getParseData();
```

#### WebURL
Represents a URL with metadata.

```java
String url = webURL.getURL();
int depth = webURL.getDepth();
WebURL parent = webURL.getParentUrl();
```

#### HtmlParseData
Parsed HTML content and links.

```java
String text = htmlParseData.getText();
String html = htmlParseData.getHtml();
Set<WebURL> links = htmlParseData.getOutgoingUrls();
```

## ğŸ§ª Testing

### Running Tests

```bash
# Run all tests
./gradlew test

# Run specific module tests
./gradlew :crawlerbykc:test

# Run with verbose output
./gradlew test --info

# Run specific test class
./gradlew test --tests "*PageFetcherTest*"
```

### Test Results
- **Core Tests**: âœ… 2/2 tests passing
- **Build Status**: âœ… Successful
- **Coverage**: Core functionality fully tested

### Writing Custom Tests

```java
@Test
public void testCustomCrawler() {
    // Your test implementation
    CrawlConfig config = new CrawlConfig();
    config.setCrawlStorageFolder("/tmp/test");
    
    // Test your crawler logic
    assertTrue(crawler.shouldVisit(null, testUrl));
}
```

## ğŸ”§ Troubleshooting

### Common Issues

#### SSL/TLS Errors
```
javax.net.ssl.SSLHandshakeException: No negotiable cipher suite
```
**Solution**: This is expected with Java 8 and modern SSL/TLS. Update to Java 11+ or configure SSL settings.

#### Memory Issues
```
OutOfMemoryError: Java heap space
```
**Solution**: Increase heap size:
```bash
export JAVA_OPTS="-Xmx2g"
```

#### Build Failures
```
Could not resolve dependencies
```
**Solution**: Check internet connection and proxy settings:
```bash
./gradlew build --refresh-dependencies
```

### Performance Optimization

#### For Large Crawls
```java
// Increase memory
config.setMaxConnectionsPerHost(200);
config.setConnectionTimeout(30000);

// Use resumable crawling
config.setResumableCrawling(true);
```

#### For High-Speed Crawling
```java
// Reduce delays (be respectful!)
config.setPolitenessDelay(100);
config.setMaxConnectionsPerHost(500);
```

### Debugging

#### Enable Debug Logging
```java
// Add to your crawler
Logger logger = LoggerFactory.getLogger(MyCrawler.class);
logger.debug("Processing URL: {}", url);
```

#### Monitor Crawl Progress
```java
@Override
public void visit(Page page) {
    // Add progress tracking
    System.out.println("Progress: " + getMyLocalData().getPagesProcessed());
}
```

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### Development Setup
1. Fork the repository
2. Clone your fork: `git clone https://github.com/your-username/crawlerbykc.git`
3. Create a feature branch: `git checkout -b feature/amazing-feature`
4. Make your changes
5. Run tests: `./gradlew test`
6. Commit your changes: `git commit -m 'Add amazing feature'`
7. Push to your fork: `git push origin feature/amazing-feature`
8. Create a Pull Request

### Code Style
- Follow Java naming conventions
- Add Javadoc comments for public methods
- Write unit tests for new features
- Ensure all tests pass

### Reporting Issues
- Use the GitHub issue tracker
- Include Java version and OS information
- Provide minimal reproduction code
- Include relevant log output

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original crawler4j project for the foundation
- Apache HttpClient for HTTP functionality
- Apache Tika for content parsing
- All contributors and users

## ğŸ“ Support

- **Documentation**: This README and inline Javadoc
- **Issues**: GitHub Issues for bug reports and feature requests
- **Discussions**: GitHub Discussions for questions and ideas

---

**Crawlerbykc** - Making web crawling simple, powerful, and reliable! ğŸ•·ï¸

---

*Last updated: October 18, 2025*
